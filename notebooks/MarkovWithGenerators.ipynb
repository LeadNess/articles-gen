{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import pymongo\n",
    "\n",
    "\n",
    "class MongoStorage:\n",
    "    \"\"\"Class for working with MongoDB\"\"\"\n",
    "\n",
    "    db: pymongo.database.Database\n",
    "    col: pymongo.collection.Collection\n",
    "\n",
    "    def __init__(self, db: pymongo.database.Database, col: pymongo.collection.Collection):\n",
    "        self.db = db\n",
    "        self.col = col\n",
    "\n",
    "    @classmethod\n",
    "    def connect(cls, host: str, port=27017, db_name='wiki', col_name='articles'):\n",
    "        db = pymongo.MongoClient(host, port, unicode_decode_error_handler='ignore')[db_name]\n",
    "        return cls(\n",
    "            db=db,\n",
    "            col=db[col_name])\n",
    "\n",
    "    def get_articles(self, count=0) -> list:\n",
    "        return list(self.col.find({}).limit(count))\n",
    "\n",
    "    def get_articles_gen(self, count=0) -> Generator:\n",
    "        return self.col.find({}).limit(count)\n",
    "\n",
    "    def get_article(self, title) -> dict:\n",
    "        doc = self.col.find_one({'title': title})\n",
    "        return doc if doc else {}\n",
    "\n",
    "    def get_articles_headings_texts(self, count=0) -> list:\n",
    "        articles = self.get_articles(count)\n",
    "        return [article['text']['Заголовок']['text'] for article in articles]\n",
    "\n",
    "    def get_articles_headings_texts_gen(self, count=0) -> list:\n",
    "        for article in self.get_articles_gen(count):\n",
    "            yield article['text']['Заголовок']['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "\n",
    "class PostgresStorage:\n",
    "\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "        self.cursor = conn.cursor()\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(host, port=5432, user='postgres', password='password', dbname='habr'):\n",
    "        return PostgresStorage(conn=psycopg2.connect(\n",
    "            host=host, port=port, user=user, password=password, dbname=dbname)\n",
    "        )\n",
    "\n",
    "    def get_posts(self, count=0) -> list:\n",
    "        if count > 0:\n",
    "            self.cursor.execute('SELECT * FROM posts LIMIT %d' % count)\n",
    "        else:\n",
    "            self.cursor.execute('SELECT * FROM posts')\n",
    "        posts = list(self.cursor.fetchall())\n",
    "        return posts\n",
    "\n",
    "    def get_posts_texts(self, count=0) -> list:\n",
    "        posts_list = self.get_posts(count)\n",
    "        return [post[2] for post in posts_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdb = MongoStorage.connect(\n",
    "            host='localhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not connect to server: No route to host\n\tIs the server running on host \"172.17.0.2\" and accepting\n\tTCP/IP connections on port 5432?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-80b5219821ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m pg = PostgresStorage.connect(\n\u001b[0;32m----> 2\u001b[0;31m             host='172.17.0.2')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0611eb8c1948>\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(host, port, user, password, dbname)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5432\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'postgres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'habr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         return PostgresStorage(conn=psycopg2.connect(\n\u001b[0;32m---> 13\u001b[0;31m             host=host, port=port, user=user, password=password, dbname=dbname)\n\u001b[0m\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: No route to host\n\tIs the server running on host \"172.17.0.2\" and accepting\n\tTCP/IP connections on port 5432?\n"
     ]
    }
   ],
   "source": [
    "pg = PostgresStorage.connect(\n",
    "            host='172.17.0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "class PostgresStorage:\n",
    "\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "        self.cursor = conn.cursor()\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(host, port=5432, user='postgres', password='password', dbname='habr'):\n",
    "        return PostgresStorage(conn=psycopg2.connect(\n",
    "            host=host, port=port, user=user, password=password, dbname=dbname)\n",
    "        )\n",
    "\n",
    "    def get_posts(self, count=0) -> list:\n",
    "        if count > 0:\n",
    "            self.cursor.execute('SELECT * FROM posts LIMIT %d' % count)\n",
    "        else:\n",
    "            self.cursor.execute('SELECT * FROM posts')\n",
    "        posts = list(self.cursor.fetchall())\n",
    "        return posts\n",
    "\n",
    "    def get_posts_texts(self, count=0) -> list:\n",
    "        posts_list = self.get_posts(count)\n",
    "        return [post[2] for post in posts_list]\n",
    "    \n",
    "    def get_posts(self, count=0) -> Generator:\n",
    "        if count > 0:\n",
    "            self.cursor.execute('SELECT * FROM posts LIMIT %d' % count)\n",
    "        else:\n",
    "            self.cursor.execute('SELECT * FROM posts')\n",
    "        posts_gen = (post for post in self.cursor.fetchall())\n",
    "        return posts_gen\n",
    "    \n",
    "    def get_posts_texts(self, count=0) -> Generator:\n",
    "        if count > 0:\n",
    "            self.cursor.execute('SELECT * FROM posts LIMIT %d' % count)\n",
    "        else:\n",
    "            self.cursor.execute('SELECT * FROM posts')\n",
    "        posts_gen = (post[2] for post in self.cursor.fetchall())\n",
    "        return posts_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PostgresStorage.connect(\n",
    "            host='172.17.0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pg.get_posts_texts(count=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from time import sleep\n",
    "for i in tqdm.tqdm([1, 2, 3, 4, 5]):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Generator\n",
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    to_sentences = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "    remove_brackets = re.compile(r' \\((.*?)\\)')\n",
    "    remove_punctuation = re.compile(r'[^a-zA-Zа-яА-Я ]')\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str, remove_punctuation=True, remove_brackets=True) -> Generator:\n",
    "        buf = text.split('\\n')\n",
    "        buf = (item for item in buf if item)\n",
    "        sentences = (sentence[:-1].lower()\n",
    "                     for sentence in cls.to_sentences.split(' '.join(buf))\n",
    "                     if sentence[:-1])\n",
    "        if remove_brackets:\n",
    "            sentences = (cls.remove_brackets.sub('', sentence) for sentence in sentences)\n",
    "        if remove_punctuation:\n",
    "            return (cls.remove_punctuation.sub('', sentence) for sentence in sentences)\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    @classmethod\n",
    "    def __get_sentences_list(cls, text_list: list, remove_punctuation=True, remove_brackets=True) -> list:\n",
    "        sentences_list = []\n",
    "        for text in text_list:\n",
    "            sentences_list += list(cls.tokenizer.tokenize(\n",
    "                text=text,\n",
    "                remove_punctuation=remove_punctuation,\n",
    "                remove_brackets=remove_brackets))\n",
    "        return sentences_list\n",
    "\n",
    "    @classmethod\n",
    "    def get_sentences_gens(cls, text_gen: Generator, remove_punctuation=True, remove_brackets=True) -> Generator:\n",
    "        for text in text_gen:\n",
    "            yield cls.tokenizer.tokenize(\n",
    "                text=text,\n",
    "                remove_punctuation=remove_punctuation,\n",
    "                remove_brackets=remove_brackets)\n",
    "\n",
    "    @classmethod\n",
    "    def process_text_list(cls, text_list: list, window_size=1) -> str:\n",
    "        text = ''\n",
    "        sentences_list = cls.__get_sentences_list(text_list)\n",
    "        for sentence_num in range(len(sentences_list)):\n",
    "            sentence = sentences_list[sentence_num]\n",
    "            for i in range(window_size):\n",
    "                text += (' '.join(sentence.split()[i:]) + '\\n')\n",
    "        return text[:-1]\n",
    "\n",
    "    @classmethod\n",
    "    def get_text_gen(cls, text_gens_gen: Generator, window_size=1) -> Generator:\n",
    "        for text_gen in text_gens_gen:\n",
    "            for sentences_gen in cls.get_sentences_gens(text_gen):\n",
    "                for sentence in sentences_gen:\n",
    "                    for ngram in (' '.join(ngram) for ngram in ngrams(sentence.split(), window_size)):\n",
    "                        yield ngram\n",
    "\n",
    "    @classmethod\n",
    "    def process_text_gen(cls, text_gens_gen: Generator, window_size=1) -> str:\n",
    "        text = ''\n",
    "        for sentence in cls.get_text_gen(\n",
    "                text_gens_gen=text_gens_gen,\n",
    "                window_size=window_size):\n",
    "            text += (sentence + '\\n')\n",
    "        return text[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import mongo, postgres\n",
    "\n",
    "def get_ram_model(\n",
    "        mongo_storage: mongo.MongoStorage,\n",
    "        postgres_storage: postgres.PostgresStorage,\n",
    "        wiki_articles_count=1000,\n",
    "        habr_posts_count=1000,\n",
    "        model_state=3\n",
    "):\n",
    "    habr_posts = postgres_storage.get_posts_texts(\n",
    "        count=habr_posts_count)\n",
    "    wiki_articles = mongo_storage.get_articles_headings_texts(\n",
    "        count=wiki_articles_count)\n",
    "    input_text = TextProcessor.process_text_list(\n",
    "        text_list=habr_posts + wiki_articles,\n",
    "        window_size=model_state)\n",
    "    model = MarkovModel(input_text, state_size=model_state)\n",
    "    return model.compile()\n",
    "\n",
    "\n",
    "def get_ram_model(\n",
    "        mongo_storage: mongo.MongoStorage,\n",
    "        postgres_storage: postgres.PostgresStorage,\n",
    "        wiki_articles_count=1000,\n",
    "        habr_posts_count=1000,\n",
    "        model_state=3\n",
    "):\n",
    "    habr_posts_gen = postgres_storage.get_posts_texts(\n",
    "        count=habr_posts_count)\n",
    "    wiki_articles_gen = mongo_storage.get_articles_headings_texts_gen(\n",
    "        count=wiki_articles_count)\n",
    "    text_gen = TextProcessor.get_text_gen(\n",
    "        text_gens_gen=(text_gen for text_gen in (habr_posts_gen, wiki_articles_gen)),\n",
    "        window_size=model_state)\n",
    "    model = MarkovModel(text_gen, state_size=model_state)\n",
    "    return model.compile()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.markov.train import get_ram_model, get_ram_model\n",
    "from models.utils import mongo, postgres\n",
    "from models.markov.markov_model import MarkovModel\n",
    "\n",
    "__mongo_storage: mongo.MongoStorage = None\n",
    "__postgres_storage: postgres.PostgresStorage = None\n",
    "__model: MarkovModel = None\n",
    "\n",
    "\n",
    "def __get_wiki_storage():\n",
    "    global __mongo_storage\n",
    "    if __mongo_storage:\n",
    "        return __mongo_storage\n",
    "    else:\n",
    "        __mongo_storage = mongo.MongoStorage.connect(\n",
    "            host='localhost')\n",
    "        return __mongo_storage\n",
    "\n",
    "\n",
    "def get_habr_storage():\n",
    "    global __postgres_storage\n",
    "    if __postgres_storage:\n",
    "        return __postgres_storage\n",
    "    else:\n",
    "        __postgres_storage = postgres.PostgresStorage.connect(\n",
    "            host='172.17.0.2')\n",
    "        return __postgres_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Generator\n",
    "import markovify\n",
    "\n",
    "\n",
    "class MarkovModel:\n",
    "\n",
    "    model: markovify.Text\n",
    "\n",
    "    def __init__(self, train_input, model=None, state_size=2):\n",
    "        if model:\n",
    "            self.model = model\n",
    "        elif isinstance(train_input, str):\n",
    "            start = time.time()\n",
    "            print(\"Start training markovify.NewlineText\")\n",
    "            self.model = markovify.NewlineText(train_input, state_size=state_size)\n",
    "            print(\"Finish training: \", time.time() - start)\n",
    "        elif isinstance(train_input, Generator):\n",
    "            self.model = markovify.Text(next(train_input), state_size=state_size)\n",
    "            for sent in tqdm.tqdm(train_input):\n",
    "                self.model = markovify.combine([self.model, markovify.Text(sent, state_size=state_size)])\n",
    "\n",
    "    def compile(self):\n",
    "        self.model.compile(inplace=True)\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name='model1.0-habr-10000.json', models_path='models/markov/bin'):\n",
    "        with open(os.path.join(models_path, model_name), 'r') as f:\n",
    "            model_json = f.read()\n",
    "        model = markovify.Text.from_json(model_json)\n",
    "        return cls(model=model)\n",
    "\n",
    "    def save(self, model_name):\n",
    "        with open(f'models/markov/bin/{model_name}.json', 'w') as f:\n",
    "            f.write(self.model.to_json())\n",
    "\n",
    "    def generate_sample(self, beginning: str) -> str:\n",
    "        return self.model.make_sentence_with_start(beginning)\n",
    "\n",
    "    def make_sentences_for_t9(self, beginning: str, count=20) -> list:\n",
    "        phrases = set()\n",
    "        for i in range(count):\n",
    "            phrase = self.generate_sample(beginning)\n",
    "            if phrase:\n",
    "                words_list = phrase.split()\n",
    "                if len(words_list) > 1:\n",
    "                    phrases.add(\" \".join(words_list[1:6]))\n",
    "        return list(phrases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not connect to server: No route to host\n\tIs the server running on host \"172.17.0.2\" and accepting\n\tTCP/IP connections on port 5432?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9543a02e3fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = get_ram_model(\n\u001b[1;32m      3\u001b[0m             \u001b[0mmongo_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__get_mongo_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mpostgres_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m__get_postgres_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mmodel_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mwiki_articles_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c217562e9522>\u001b[0m in \u001b[0;36m__get_postgres_storage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         __postgres_storage = postgres.PostgresStorage.connect(\n\u001b[0;32m---> 26\u001b[0;31m             host='172.17.0.2')\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__postgres_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/articles-gen/models/utils/postgres.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(host, port, user, password, dbname)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5432\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'postgres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'habr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         return PostgresStorage(conn=psycopg2.connect(\n\u001b[0;32m---> 14\u001b[0;31m             host=host, port=port, user=user, password=password, dbname=dbname)\n\u001b[0m\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: No route to host\n\tIs the server running on host \"172.17.0.2\" and accepting\n\tTCP/IP connections on port 5432?\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "model = get_ram_model(\n",
    "            mongo_storage=__get_wiki_storage(),\n",
    "            postgres_storage=get_habr_storage(),\n",
    "            model_state=3,\n",
    "            wiki_articles_count=1000,\n",
    "            habr_posts_count=1000\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = 3\n",
    "habr_posts_count = 1000\n",
    "wiki_articles_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old variant - without generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Get posts:  0.14567303657531738\n",
      "Get articles:  0.40266871452331543\n",
      "Processed text:  2.211005449295044\n",
      "Start training markovify.NewlineText\n",
      "Finish training:  7.211528778076172\n",
      "Finish:  13.537837028503418\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Start!\")\n",
    "habr_posts = pg.get_posts_texts(\n",
    "    count=habr_posts_count)\n",
    "print(\"Get posts: \", time.time() - start)\n",
    "wiki_articles = mdb.get_articles_headings_texts(\n",
    "    count=wiki_articles_count)\n",
    "print(\"Get articles: \", time.time() - start)\n",
    "input_text = TextProcessor.process_text_list(\n",
    "    text_list=habr_posts + wiki_articles,\n",
    "    window_size=model_state)\n",
    "print(\"Processed text: \", time.time() - start)\n",
    "model = MarkovModel(input_text, state_size=model_state).compile()\n",
    "print(\"Finish: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New variant - with generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "Get posts gen:  0.14258933067321777\n",
      "Get articles gen:  0.14279603958129883\n",
      "Processed text gens:  2.6517739295959473\n",
      "Start training markovify.NewlineText\n",
      "Finish training:  12.632851839065552\n",
      "Finish:  23.218796968460083\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Start!\")\n",
    "habr_posts_gen = pg.get_posts_texts(\n",
    "    count=habr_posts_count)\n",
    "print(\"Get posts gen: \", time.time() - start)\n",
    "wiki_articles_gen = mdb.get_articles_headings_texts_gen(\n",
    "    count=wiki_articles_count)\n",
    "print(\"Get articles gen: \", time.time() - start)\n",
    "text_gen = TextProcessor.process_text_gen(\n",
    "    text_gens_gen=(text_gen for text_gen in (habr_posts_gen, wiki_articles_gen)),\n",
    "    window_size=model_state)\n",
    "print(\"Processed text gens: \", time.time() - start)\n",
    "model = MarkovModel(text_gen, state_size=model_state).compile()\n",
    "print(\"Finish: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "habr_posts = pg.get_posts_texts(\n",
    "        count=1000)\n",
    "wiki_articles = mdb.get_articles_headings_texts(\n",
    "    count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "habr_posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = (i.strip() for i in\" \".join(map(lambda x:\n",
    "       x.replace(\"\\n\", \" \")\n",
    "        .replace(\".\", \" \")\n",
    "        .replace(\"(\", \" \")\n",
    "        .replace(\")\", \" \")\n",
    "        .replace(\",\", \" \")\n",
    "        .replace(\";\", \" \")\n",
    "        .replace(\":\", \" \")\n",
    "        .replace(\"'\", \" \")\n",
    "        .replace('\"', \" \")\n",
    "        .replace(\"?\", \" \")\n",
    "        .replace(\"!\", \" \")\n",
    "        .replace(\"»\", \" \")\n",
    "        .replace(\"«\", \" \")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"  \", \" \")\n",
    "        .lower(),\n",
    "        habr_posts\n",
    "      )).replace(\"  \", \" \").split(\" \") if i != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(src:list, N:int=3):\n",
    "    return zip(*(islice(seq, index, None) for index, seq in enumerate(tee(src, N))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ngrams(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = \"\\n\".join(\" \".join(x) for x in gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2523it [00:03, 640.12it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('___BEGIN__', '___BEGIN__')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-07b89478b1d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkovify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkovify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_text, state_size, chain, parsed_sentences, retain_original, well_formed, reject_reg)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Rejoined text lets us assess the novelty of generated sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrejoined_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_join\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, state_size, model)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBEGIN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompute_begin_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36mprecompute_begin_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m     97\u001b[0m         \u001b[0mbegin_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mBEGIN\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbegin_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_cumdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcumdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_choices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('___BEGIN__', '___BEGIN__')"
     ]
    }
   ],
   "source": [
    "sentence_gen = (\" \".join(x) for x in gen)\n",
    "model = markovify.Text(next(sentence_gen))\n",
    "max_it = 10000\n",
    "for it, sent in enumerate(tqdm.tqdm(sentence_gen)):\n",
    "    if it > max_it:\n",
    "        break\n",
    "    model = markovify.combine([model, markovify.Text(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<markovify.text.Text at 0x7f67f9438510>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895665"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'уже 29 октября\\n29 октября в\\nоктября в олимпийском\\nв олимпийском пройдут\\nолимпийском пройдут бои\\nпрой'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'новое предложение для тестов новое предложение для тестов новое предложение для тестов'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.make_sentence_with_start(\"новое\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>несколько основных\n",
      "несколько основных идей родившихся по результатам первых опытов\n",
      ">>>>первых\n",
      "None\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость робота от\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость робота от\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость робота от\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "None\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость робота от переворота\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "первых опытов боёв это\n",
      ">>>>первых\n",
      "None\n",
      ">>>>первых\n",
      "первых опытов боёв это независимость\n",
      ">>>>это\n",
      "это новый робот открытые колеса сверху 10 rip великобритания маленький и юркий\n",
      ">>>>этот\n",
      "этот раз в списке снова\n",
      ">>>>эти\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('___BEGIN__', 'эти')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-7372c24a89ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>>>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_sentence_with_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/text.py\u001b[0m in \u001b[0;36mmake_sentence_with_start\u001b[0;34m(self, beginning, strict, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minit_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/text.py\u001b[0m in \u001b[0;36mmake_sentence\u001b[0;34m(self, init_state, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin_words\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(self, init_state)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36mgen\u001b[0;34m(self, init_state)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBEGIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnext_word\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEND\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mcumdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_cumdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mcumdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcumdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('___BEGIN__', 'эти')"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    text = input(\">>>>\").strip()\n",
    "#     print(text)\n",
    "    print(model.make_sentence_with_start(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from time import sleep\n",
    "for i in tqdm.tqdm([1, 2, 3, 4, 5]):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    to_sentences = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "    remove_brackets = re.compile(r' \\((.*?)\\)')\n",
    "    remove_punctuation = re.compile(r'[^a-zA-Zа-яА-Я ]')\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str, remove_punctuation=True, remove_brackets=True, remove_endings=True) -> list:\n",
    "        buf = text.split('\\n')\n",
    "        buf = [item for item in buf if item]\n",
    "        sentences = []\n",
    "        if not remove_endings:\n",
    "            for sentence in cls.to_sentences.split(' '.join(buf)):\n",
    "                buf_list = [item for item in sentence.split('!') if item]\n",
    "                for item in buf_list:\n",
    "                    if item[-1] == '.' or item[-1] == '?':\n",
    "                        sentences.append(item.lower())\n",
    "                    else:\n",
    "                        sentences.append(item.lower() + '!')\n",
    "        else:\n",
    "            sentences = [sentence[:-1].lower() for sentence in cls.to_sentences.split(' '.join(buf)) if sentence[:-1]]\n",
    "        if remove_brackets:\n",
    "            sentences = [cls.remove_brackets.sub('', sentence) for sentence in sentences]\n",
    "        if remove_punctuation:\n",
    "            return [cls.remove_punctuation.sub('', sentence) for sentence in sentences]\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    @classmethod\n",
    "    def __get_sentences_list(cls, text_list: list, remove_brackets=True, remove_endings=True) -> list:\n",
    "        sentences_list = []\n",
    "        for text in text_list:\n",
    "            sentences_list += cls.tokenizer.tokenize(\n",
    "                text=text,\n",
    "                remove_brackets=remove_brackets,\n",
    "                remove_endings=remove_endings)\n",
    "        return sentences_list\n",
    "\n",
    "    @classmethod\n",
    "    def process_text_list(cls, text_list: list, window_size=1, remove_brackets=True, remove_endings=True) -> str:\n",
    "        text = ''\n",
    "        sentences_list = cls.__get_sentences_list(\n",
    "            text_list=text_list,\n",
    "            remove_brackets=remove_brackets,\n",
    "            remove_endings=remove_endings)\n",
    "        for sentence_num in range(len(sentences_list)):\n",
    "            sentence = sentences_list[sentence_num]\n",
    "            for i in range(window_size):\n",
    "                text += (' '.join(sentence.split()[i:]) + '\\n')\n",
    "        return text[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = TextProcessor.process_text_list(\n",
    "        text_list=habr_posts + wiki_articles,\n",
    "        window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
