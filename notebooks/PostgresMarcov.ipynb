{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text genarators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiStorage - class for working with wiki articles stored in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import pymongo\n",
    "\n",
    "\n",
    "class WikiStorage:\n",
    "    \"\"\"Class for working with MongoDB\"\"\"\n",
    "\n",
    "    db: pymongo.database.Database\n",
    "    col: pymongo.collection.Collection\n",
    "\n",
    "    def __init__(self, db: pymongo.database.Database, col: pymongo.collection.Collection):\n",
    "        self.db = db\n",
    "        self.col = col\n",
    "\n",
    "    @classmethod\n",
    "    def connect(cls, host: str, port=27017, db_name='wiki', col_name='articles'):\n",
    "        db = pymongo.MongoClient(host, port, unicode_decode_error_handler='ignore')[db_name]\n",
    "        return cls(\n",
    "            db=db,\n",
    "            col=db[col_name])\n",
    "\n",
    "    def get_articles(self, count=0) -> Generator:\n",
    "        return self.col.find({}).limit(count)\n",
    "\n",
    "    def get_article(self, title) -> dict:\n",
    "        doc = self.col.find_one({'title': title})\n",
    "        return doc if doc else {}\n",
    "\n",
    "    def get_articles_headings_texts(self, count=0) -> list:\n",
    "        for article in self.get_articles(count):\n",
    "            yield article['text']['Заголовок']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres storage - base class for all classes working with PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import psycopg2\n",
    "\n",
    "class PostgresStorage:\n",
    "    \n",
    "    conn: psycopg2.extensions.connection\n",
    "    \n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    @classmethod\n",
    "    def connect(cls, \n",
    "                host: str, \n",
    "                port: int = 5432,\n",
    "                user: str = 'postgres',\n",
    "                password: str = 'password',\n",
    "                dbname: str = 'postgres'):\n",
    "        return cls(conn=psycopg2.connect(\n",
    "            host=host, port=port, user=user, password=password, dbname=dbname)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habr storage - class for working posts from habr stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HabrStorage(PostgresStorage):\n",
    "\n",
    "    def get_posts(self, \n",
    "                  count: int = 0, \n",
    "                  habs_list: list = None, \n",
    "                  tags_list: list = None) -> Generator:\n",
    "        if not habs_list and not tags_list:\n",
    "            cursor = self.conn.cursor()\n",
    "            sql = 'SELECT * FROM posts'\n",
    "            if count:\n",
    "                sql += ' LIMIT %d' % count\n",
    "            cursor.execute(sql)\n",
    "            return (post for post in cursor.fetchall())\n",
    "        elif habs_list:\n",
    "            return self.__get_posts_by_habs(count, habs_list)\n",
    "        elif tags_list:\n",
    "            return self.__get_posts_by_tags(count, tags_list)\n",
    "\n",
    "    def get_posts_texts(self,\n",
    "                        count: int = 0,\n",
    "                        habs_list: list = None, \n",
    "                        tags_list: list = None) -> Generator:\n",
    "        posts_texts_gen = (post[2] for post in self.get_posts(count, habs_list, tags_list))\n",
    "        return posts_texts_gen\n",
    "\n",
    "    def __get_posts_by_habs(self, \n",
    "                            count: int,\n",
    "                            habs_list: list) -> Generator:\n",
    "        sql = '''SELECT P.* \n",
    "                   FROM posts P JOIN habs H ON P.post_id = H.post_id\n",
    "                  WHERE H.hab in (%s)''' % ''.join([\"'\" + str(hab) + \"', \" for hab in habs_list])[:-2]\n",
    "        sql = sql + \" LIMIT %d\" % count if count > 0 else sql\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        return (post for post in cursor.fetchall())\n",
    "\n",
    "    def __get_posts_by_tags(self, \n",
    "                            count: int,\n",
    "                            tags_list: list) -> Generator:\n",
    "        sql = '''SELECT P.* \n",
    "                   FROM posts P JOIN tags T ON P.post_id = T.post_id\n",
    "                  WHERE T.tag in (%s)''' % ''.join([\"'\" + str(tag) + \"', \" for tag in tags_list])[:-2]\n",
    "        sql = sql + \" LIMIT %d\" % count if count > 0 else sql\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        return (post for post in cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Iterable\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    to_sentences = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "    remove_brackets = re.compile(r' \\((.*?)\\)')\n",
    "    remove_punctuation = re.compile(r'[^a-zA-Zа-яА-Я ]')\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str, remove_punctuation=True, remove_brackets=True) -> Generator:\n",
    "        buf = text.split('\\n')\n",
    "        buf = (item for item in buf if item)\n",
    "        sentences = (sentence[:-1].lower().strip()\n",
    "                     for sentence in cls.to_sentences.split(' '.join(buf))\n",
    "                     if sentence[:-1])\n",
    "        if remove_brackets:\n",
    "            sentences = (cls.remove_brackets.sub('', sentence) for sentence in sentences)\n",
    "        if remove_punctuation:\n",
    "            return (cls.remove_punctuation.sub('', sentence) for sentence in sentences)\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    @classmethod\n",
    "    def get_sentences_gens(cls, texts: Iterable, remove_punctuation=True, remove_brackets=True) -> Generator:\n",
    "        for text in texts:\n",
    "            yield cls.tokenizer.tokenize(\n",
    "                text=text,\n",
    "                remove_punctuation=remove_punctuation,\n",
    "                remove_brackets=remove_brackets)\n",
    "\n",
    "    @classmethod\n",
    "    def get_text_gen(cls, text_gens_gen: Iterable) -> Generator:\n",
    "        for text_gen in text_gens_gen:\n",
    "            for sentences_gen in cls.get_sentences_gens(text_gen):\n",
    "                for sentence in sentences_gen:\n",
    "                    yield sentence.split()\n",
    "\n",
    "    @classmethod\n",
    "    def get_ngram_gen(cls, text_gens_gen: Iterable, ngram_size: int = 3) -> Generator:\n",
    "        for text_gen in text_gens_gen:\n",
    "            for sentences_gen in cls.get_sentences_gens(text_gen):\n",
    "                for sentence in sentences_gen:\n",
    "                    yield [''.join(item) for item in nltk.ngrams(sentence, ngram_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words encoder - class for encoding/decoding words stored as int nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Generator\n",
    "        \n",
    "class WordsEncoder:\n",
    "    \n",
    "    counter: int\n",
    "    word2int: dict\n",
    "    int2word: dict\n",
    "    begin_word: int = 0\n",
    "    end_word: int = -1\n",
    "        \n",
    "    def __init__(self, counter: int, word2int: dict, int2word: dict):\n",
    "        self.counter = counter\n",
    "        self.word2int = word2int\n",
    "        self.int2word = int2word\n",
    "\n",
    "    def fit(self, text_corpus):\n",
    "        self.counter = 0\n",
    "        self.word2int = {\n",
    "            self.begin_word: self.begin_word,\n",
    "            self.end_word: self.end_word\n",
    "        }\n",
    "        self.int2word = {\n",
    "            self.begin_word: self.begin_word,\n",
    "            self.end_word: self.end_word\n",
    "        }\n",
    "        for sentence in text_corpus:\n",
    "            for word in sentence.split():\n",
    "                if word not in self.word2int:\n",
    "                    self.counter += 1\n",
    "                    self.word2int[word] = self.counter\n",
    "                    self.int2word[self.counter] = word\n",
    "                    \n",
    "    def fit_encode(self, text_corpus) -> list:\n",
    "        corpus = list(text_corpus) if isinstance(text_corpus, Generator) else text_corpus\n",
    "        self.fit(corpus)\n",
    "        return self.encode_text_corpus_gen(corpus)\n",
    "\n",
    "    def encode_words_list(self, words_list: list) -> list:\n",
    "        return [self.word2int[word] for word in words_list]\n",
    "\n",
    "    def encode_text_corpus(self, text_corpus: list) -> list:\n",
    "        \"\"\"List of lists of words\"\"\"\n",
    "        return [self.encode_words_list(words_list) for words_list in text_corpus]\n",
    "\n",
    "    def encode_text_corpus_gen(self, text_corpus_gen: Generator) -> Generator:\n",
    "        \"\"\"List of lists of words\"\"\"\n",
    "        return (self.encode_words_list(sentence.split()) for sentence in text_corpus_gen)\n",
    "\n",
    "    def decode_codes_list(self, codes_list: list) -> list:\n",
    "        return [self.int2word[code] for code in codes_list]\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Returns the underlying data as a Python dict.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"counter\": self.counter,\n",
    "            \"word2int\": self.word2int,\n",
    "            \"int2word\": self.int2word\n",
    "        }\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"\n",
    "        Returns the underlying data as a JSON string.\n",
    "        \"\"\"\n",
    "        return json.dumps(self.to_dict())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, obj):\n",
    "\n",
    "        int2word = obj[\"int2word\"]\n",
    "        for key in int2word:\n",
    "            int2word[int(key)] = int2word.pop(key)\n",
    "\n",
    "        int2word[cls.end_word] = cls.end_word\n",
    "        int2word[cls.begin_word] = cls.begin_word\n",
    "\n",
    "        word2int = obj[\"word2int\"]\n",
    "        word2int[cls.end_word] = int(word2int.pop(str(cls.end_word)))\n",
    "        word2int[cls.begin_word] = int(word2int.pop(str(cls.begin_word)))\n",
    "\n",
    "        return cls(\n",
    "            counter=obj[\"counter\"],\n",
    "            word2int=word2int,\n",
    "            int2word=int2word\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        return cls.from_dict(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder storage - class for working with words encoder stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStorage(PostgresStorage):\n",
    "    \n",
    "    model_name: str\n",
    "    begin_word: int = 0\n",
    "    end_word: int = -1\n",
    "        \n",
    "    def add_encoder(self, model_name: str, encoder: WordsEncoder):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL add_encoder(%s)', [model_name])\n",
    "        self.conn.commit()\n",
    "        \n",
    "        for code, word in encoder.int2word.items():\n",
    "            sql = f'''INSERT INTO {model_name}_encoder(code, word)\n",
    "                      VALUES (%s, %s)'''\n",
    "            cursor.execute(sql, [code, word])           \n",
    "        self.conn.commit()\n",
    "        self.__create_indexes(model_name)        \n",
    "            \n",
    "    def delete_encoder(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL delete_encoder(%s)', [model_name])\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def load_encoder(self, model_name: str) -> WordsEncoder:\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(f'SELECT code, word FROM {model_name}_encoder')\n",
    "        int2word = {}\n",
    "        word2int = {}\n",
    "        for row in cursor.fetchall():\n",
    "            code, word = row[0], row[1]\n",
    "            int2word[code] = word\n",
    "            word2int[word] = code\n",
    "        word2int[self.end_word] = int(word2int.pop(str(self.end_word)))\n",
    "        word2int[self.begin_word] = int(word2int.pop(str(self.begin_word)))\n",
    "        counter = len(int2word) - 2 # except begin and end words\n",
    "        return WordsEncoder(counter=counter,\n",
    "                            int2word=int2word,\n",
    "                            word2int=word2int)\n",
    "        \n",
    "    def __create_indexes(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL create_encoder_indexes(%s)', [model_name]);\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def __drop_indexes(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL drop_encoder_indexes(%s)', [model_name]);\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain storage - realization of markov chain stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "import bisect\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "def accumulate(iterable, func=operator.add):\n",
    "    it = iter(iterable)\n",
    "    total = next(it)\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = func(total, element)\n",
    "        yield total\n",
    "\n",
    "def compile_next(next_dict):\n",
    "    words = list(next_dict.keys())\n",
    "    cff = list(accumulate(next_dict.values()))\n",
    "    return [words, cff]\n",
    "\n",
    "\n",
    "class ChainStorage(PostgresStorage):\n",
    "    \n",
    "    begin_word: int = 0\n",
    "    end_word: int = -1\n",
    "        \n",
    "    def add_model(self, model_name: str, train_corpus: list, state_size: int):\n",
    "        model_dict = self.__build_model(train_corpus, state_size)\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL add_model(%s, %s)', [model_name, self.end_word])\n",
    "        self.conn.commit()\n",
    "        \n",
    "        for state_tuple in model_dict:\n",
    "            buf = model_dict[state_tuple]\n",
    "            choices_list, cumdist_list = buf[0], buf[1] \n",
    "            self.__add_state(cursor, model_name, state_tuple, choices_list, cumdist_list)            \n",
    "        self.conn.commit()\n",
    "        self.__create_index(model_name)\n",
    "        \n",
    "        del model_dict\n",
    "            \n",
    "    def delete_model(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL delete_model(%s)', [model_name])\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def walk(self, model_name: str, init_state: list, phrase_len: int = 10):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(f'SELECT chain_walk_{model_name}(%s, %s)', [init_state, phrase_len])\n",
    "        return cursor.fetchone()[0] or []\n",
    "    \n",
    "    def __build_model(self, train_corpus, state_size: int) -> dict:\n",
    "        model = {}\n",
    "\n",
    "        for run in train_corpus:\n",
    "            items = ([ self.begin_word ] * state_size) + run + [ self.end_word ]\n",
    "            for i in range(len(run) + 1):\n",
    "                state = tuple(items[i:i+state_size])\n",
    "                follow = items[i+state_size]\n",
    "                if state not in model:\n",
    "                    model[state] = {}\n",
    "\n",
    "                if follow not in model[state]:\n",
    "                    model[state][follow] = 0\n",
    "\n",
    "                model[state][follow] += 1\n",
    "                \n",
    "        model = { state: compile_next(next_dict) for (state, next_dict) in model.items() }\n",
    "        return model\n",
    "    \n",
    "    def __add_state(self,\n",
    "            cursor,\n",
    "            model_name: str,\n",
    "            state: tuple, \n",
    "            choices: list, \n",
    "            cumdist: list):\n",
    "        sql = f'''INSERT INTO {model_name}(state, choices, cumdist)\n",
    "                  VALUES (%s, %s, %s)\n",
    "                  ON CONFLICT DO NOTHING'''\n",
    "        cursor.execute(sql, [list(state), choices, cumdist])\n",
    "    \n",
    "    def __create_index(self, model_name: str, hash_index: bool = True):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL create_model_table_index(%s, %s)', [model_name, hash_index]);\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def __drop_index(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL drop_model_table_index(%s)', [model_name]);\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_corpus = [\n",
    "    [1, 5, 6],\n",
    "    [65, 4, 1, 54],\n",
    "    [5, 65, 1, 324],\n",
    "    [3, 6, 54]\n",
    "]\n",
    "\n",
    "pg_model = ChainStorage.connect('172.17.0.2', dbname='markov')\n",
    "pg_model.add_model('test_sample', train_corpus=encoded_corpus, state_size=2)\n",
    "pg_model.walk('test_sample', [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_model.delete_model('test_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres chain usage with text processor & text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['возможности',\n",
       " 'linkedin',\n",
       " 'профиль',\n",
       " 'теперь',\n",
       " 'профиль',\n",
       " 'возможности',\n",
       " 'появившейся',\n",
       " 'кнопки',\n",
       " 'образовательного',\n",
       " 'политики']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_habs = HabrStorage.connect('172.17.0.3', dbname='habr')\n",
    "\n",
    "texts_list = list(pg_habs.get_posts_texts(10))\n",
    "train_corpus = list(TextProcessor.get_text_gen([texts_list,]))\n",
    "\n",
    "encoder = WordsEncoder()\n",
    "encoded_train_corpus = encoder.fit_encode(train_corpus)\n",
    "\n",
    "pg_model = ChainStorage.connect('172.17.0.2', dbname='markov')\n",
    "pg_model.add_model('another_test_sample', train_corpus=encoded_train_corpus, state_size=2)\n",
    "\n",
    "encoder.decode_codes_list(pg_model.walk('another_test_sample', [0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_model.delete_model('another_test_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generator model based on encoded markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterable\n",
    "\n",
    "class TextGenerator:\n",
    "    pg_chain: ChainStorage\n",
    "    pg_encoder: EncoderStorage\n",
    "    encoder: WordsEncoder\n",
    "    model_name: str\n",
    "    state_size: int\n",
    "    re_process: re.Pattern = re.compile(r'[^a-zA-Zа-яА-Я ]')\n",
    "\n",
    "    def __init__(self,\n",
    "                 pg_chain: ChainStorage,\n",
    "                 pg_encoder: EncoderStorage,\n",
    "                 model_name: str,\n",
    "                 state_size: int,\n",
    "                 input_text: Iterable = None,\n",
    "                 use_ngrams: bool = False,\n",
    "                 ngram_size: int = None):\n",
    "        self.pg_chain = pg_chain\n",
    "        self.pg_encoder = pg_encoder\n",
    "        self.model_name = model_name\n",
    "        self.state_size = state_size\n",
    "\n",
    "        if input_text:\n",
    "            self.encoder = WordsEncoder()\n",
    "            \n",
    "            train_corpus = list(TextProcessor.get_text_gen(input_text))\n",
    "            encoded_train_corpus = self.encoder.fit_encode(train_corpus)\n",
    "\n",
    "            self.pg_encoder.add_encoder(model_name, self.encoder)\n",
    "            self.pg_chain.add_model(model_name, encoded_train_corpus, state_size)\n",
    "        else:\n",
    "            self.encoder = self.pg_encoder.load_encoder(model_name)\n",
    "\n",
    "    def words_split(self, sentence: str) -> list:\n",
    "        words_list = []\n",
    "        for word in sentence.split():\n",
    "            processed_word = self.re_process.sub('', word.lower())\n",
    "            if processed_word:\n",
    "                words_list.append(processed_word)\n",
    "        return words_list\n",
    "\n",
    "    def res_join(self, words_list: list) -> str:\n",
    "        return ' '.join(words_list)\n",
    "\n",
    "    def make_sentence(self, init_state: list, **kwargs):\n",
    "        tries = kwargs.get('tries', 10)\n",
    "        max_words = kwargs.get('max_words', None)\n",
    "        min_words = kwargs.get('min_words', None)\n",
    "\n",
    "        if init_state is not None:\n",
    "            init_state = self.encoder.encode_words_list(init_state)\n",
    "            prefix = init_state\n",
    "            for word in prefix:\n",
    "                if word == self.encoder.begin_word:\n",
    "                    prefix = prefix[1:]\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            prefix = []\n",
    "\n",
    "        for _ in range(tries):\n",
    "            codes_list = prefix + self.pg_chain.walk(self.model_name, init_state)\n",
    "            words_list = self.encoder.decode_codes_list(codes_list)\n",
    "            if (max_words is not None and len(words_list) > max_words) or (\n",
    "                    min_words is not None and len(words_list) < min_words):\n",
    "                continue\n",
    "            return self.res_join(words_list)\n",
    "        return None\n",
    "\n",
    "    def make_sentence_with_start(self, input_phrase: str, **kwargs):\n",
    "        words_list = self.words_split(input_phrase)\n",
    "        words_count = len(words_list)\n",
    "\n",
    "        if words_count == self.state_size:\n",
    "            init_state = words_list\n",
    "\n",
    "        elif 0 < words_count < self.state_size:\n",
    "            init_state = [self.encoder.begin_word] * (self.state_size - words_count) + words_list\n",
    "        else:\n",
    "            init_state = [self.encoder.begin_word] * self.state_size\n",
    "\n",
    "        return self.make_sentence(init_state, **kwargs)\n",
    "\n",
    "    def make_sentences_for_t9(self, beginning: str, first_words_count=1, count=20) -> list:\n",
    "        phrases = set()\n",
    "        for i in range(count):\n",
    "            phrase = self.make_sentence_with_start(beginning)\n",
    "            if phrase:\n",
    "                words_list = phrase.split()\n",
    "                if len(words_list) > 1:\n",
    "                    phrases.add(\" \".join(words_list[first_words_count:]))\n",
    "        return list(phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_gen(\n",
    "        mongo_wiki: WikiStorage,\n",
    "        pg_habr: HabrStorage,\n",
    "        wiki_articles_count=1000,\n",
    "        habr_posts_count=1000,\n",
    "        **kwargs\n",
    "):\n",
    "    habr_posts_gen = pg_habr.get_posts_texts(\n",
    "        count=habr_posts_count, habs_list=kwargs.get('habs_list'), tags_list=kwargs.get('tags_list'))\n",
    "    wiki_articles_gen = mongo_wiki.get_articles_headings_texts(count=wiki_articles_count)\n",
    "    return (text_gen for text_gen in (habr_posts_gen, wiki_articles_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connections to dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_wiki = WikiStorage.connect(host='localhost')\n",
    "pg_habr = HabrStorage.connect(host='172.17.0.3', dbname='habr')\n",
    "pg_chain = ChainStorage.connect(host='172.17.0.2', dbname='markov')\n",
    "pg_encoder = EncoderStorage.connect(host='172.17.0.2', dbname='markov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 13.7 s, total: 34.7 s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_corpus_gen = get_text_gen(mongo_wiki=mongo_wiki,\n",
    "                                pg_habr=pg_habr,\n",
    "                                wiki_articles_count=100,\n",
    "                                habr_posts_count=200,\n",
    "                                habs_list=['Машинное обучение'])\n",
    "model = TextGenerator(pg_chain=pg_chain,\n",
    "                      pg_encoder=pg_encoder,\n",
    "                      model_name='test_model',\n",
    "                      state_size=3,\n",
    "                      input_text=train_corpus_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(pg_chain=pg_chain,\n",
    "                      pg_encoder=pg_encoder,\n",
    "                      model_name='test_model',\n",
    "                      state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'за последние годы и самая большая за два десятилетия',\n",
       " 'data engineering становится все более популярным методом и находит свое',\n",
       " 'представляю вашему вниманию вторую часть статьи о поиске подозреваемых в',\n",
       " 'за последние годы и самая большая по водности площади бассейна',\n",
       " 'задача снижения размерности является одной из важнейших в машинном обучении',\n",
       " 'мы уже говорили с theano но со временем города стали',\n",
       " 'давайте вернемся к периодически затрагиваемой у нас теме машинного обучения']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.make_sentences_for_t9('привет хабр', first_words_count=2, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dat', 'ata', 'ta ', 'a e', ' en', 'eng', 'ngi', 'gin', 'ine', 'nee', 'eer', 'eri', 'rin', 'ing', 'ng ', 'g с', ' ст', 'ста', 'тан', 'ано', 'нов', 'ови', 'вит', 'итс', 'тся', 'ся ', 'я в', ' вс', 'все', 'се ', 'е б', ' бо', 'бол', 'оле', 'лее', 'ее ', 'е с', ' сл', 'сло', 'лож', 'ожн', 'жны', 'ным']\n",
      "['за ', 'а п', ' по', 'пос', 'осл', 'сле', 'лед', 'едн', 'дни', 'ние', 'ие ', 'е г', ' го', 'год', 'оды', 'ды ', 'ы и', ' и ', 'и с', ' са', 'сам', 'ама', 'мая', 'ая ', 'я б', ' бо', 'бол', 'оль', 'льш', 'ьша', 'шая', 'ая ', 'я п', ' по', 'по ', 'о в', ' во', 'вод', 'одн', 'дно', 'нос', 'ост', 'сти', 'ти ', 'и п', ' пл', 'пло', 'лощ', 'оща', 'щад', 'ади', 'ди ', 'и б', ' ба', 'бас', 'асс', 'ссе', 'сей', 'ейн', 'йна']\n",
      "['пре', 'ред', 'едс', 'дст', 'ста', 'тав', 'авл', 'вля', 'ляю', 'яю ', 'ю в', ' ва', 'ваш', 'аше', 'шем', 'ему', 'му ', 'у в', ' вн', 'вни', 'ним', 'има', 'ман', 'ани', 'нию', 'ию ', 'ю п', ' пе', 'пер', 'ере', 'рев', 'евд', 'вд ', 'д с', ' ст', 'ста', 'тат', 'ать', 'тьи', 'ьи ', 'и s', ' so', 'sol', 'olv', 'lvi', 'vin', 'ing', 'ng ', 'g m', ' mu', 'mul', 'ult', 'lti', 'tia', 'iar', 'arm', 'rme', 'med', 'ed ', 'd b', ' ba', 'ban', 'and', 'ndi', 'dit', 'its', 'ts ', 's a', ' a ', 'a c', ' co', 'com', 'omp', 'mpa', 'par', 'ari', 'ris', 'iso', 'son']\n",
      "['сег', 'его', 'год', 'одн', 'дня', 'ня ', 'я м', ' мы', 'мы ', 'ы х', ' хо', 'хот', 'оти', 'тим', 'им ', 'м п', ' по', 'под', 'оде', 'дел', 'ели', 'лит', 'ить', 'тьс', 'ься', 'ся ', 'я с', ' с ', 'с в', ' ва', 'вам', 'ами', 'ми ', 'и и', ' ин', 'инс', 'нст', 'стр', 'тру', 'рук', 'укц', 'кци', 'цие', 'ией', 'ей ', 'й п', ' по', 'по ', 'о с', ' со', 'соз', 'озд', 'зда', 'дан', 'ани', 'нию', 'ию ', 'ю б', ' бо', 'бот', 'ота']\n",
      "['зад', 'ада', 'дач', 'ача', 'ча ', 'а с', ' сн', 'сни', 'ниж', 'иже', 'жен', 'ени', 'ния', 'ия ', 'я р', ' ра', 'раз', 'азм', 'зме', 'мер', 'ерн', 'рно', 'нос', 'ост', 'сти', 'ти ', 'и я', ' яв', 'явл', 'вля', 'ляе', 'яет', 'етс', 'тся', 'ся ', 'я о', ' од', 'одн', 'дно', 'ной', 'ой ', 'й и', ' из', 'из ', 'з з', ' зе', 'зем', 'еме', 'мел', 'ель', 'ль ', 'ь ф', ' фе', 'фед', 'еде', 'дер', 'ера', 'рат', 'ати', 'тив', 'ивн', 'вно', 'ной', 'ой ', 'й р', ' ре', 'рес', 'есп', 'спу', 'пуб', 'убл', 'бли', 'лик', 'ики', 'ки ', 'и г', ' ге', 'гер', 'ерм', 'рма', 'ман', 'ани', 'ния']\n",
      "['мы ', 'ы у', ' уж', 'уже', 'же ', 'е г', ' го', 'гов', 'ово', 'вор', 'ори', 'рил', 'или', 'ли ', 'и с', ' с ', 'с t', ' th', 'the', 'hea', 'ean', 'ano', 'no ', 'o н', ' но', 'но ', 'о с', ' со', 'со ', 'о в', ' вр', 'вре', 'рем', 'еме', 'мен', 'ене', 'нем', 'ем ', 'м г', ' го', 'гор', 'оро', 'род', 'ода', 'да ', 'а с', ' ст', 'ста', 'тал', 'али']\n",
      "['сег', 'его', 'год', 'одн', 'дня', 'ня ', 'я м', ' мы', 'мы ', 'ы х', ' хо', 'хот', 'оти', 'тим', 'им ', 'м п', ' по', 'пог', 'ого', 'гов', 'ово', 'вор', 'ори', 'рит', 'ить', 'ть ', 'ь о', ' о ', 'о к', ' ко', 'кон', 'онц', 'нце', 'цеп', 'епц', 'пци', 'ции', 'ии ', 'и i', ' in', 'ins', 'nsi', 'sig', 'igh', 'ght', 'htd', 'tdr', 'dri', 'riv', 'ive', 'ven', 'en ', 'n и', ' и ', 'и о', ' о ', 'о т', ' то', 'том']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "ngram_size = 3\n",
    "corpus = ['data engineering становится все более сложным',\n",
    " 'за последние годы и самая большая по водности площади бассейна',\n",
    " 'представляю вашему вниманию перевд статьи solving multiarmed bandits a comparison',\n",
    " 'сегодня мы хотим поделиться с вами инструкцией по созданию бота',\n",
    " 'задача снижения размерности является одной из земель федеративной республики германия',\n",
    " 'мы уже говорили с theano но со временем города стали',\n",
    " 'сегодня мы хотим поговорить о концепции insightdriven и о том']\n",
    "\n",
    "for sentence in corpus[:2]:\n",
    "    print([''.join(item) for item in nltk.ngrams(sentence, ngram_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
