{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text genarators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiStorage - class for working with wiki articles stored in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import pymongo\n",
    "\n",
    "\n",
    "class WikiStorage:\n",
    "    \"\"\"Class for working with MongoDB\"\"\"\n",
    "\n",
    "    db: pymongo.database.Database\n",
    "    col: pymongo.collection.Collection\n",
    "\n",
    "    def __init__(self, db: pymongo.database.Database, col: pymongo.collection.Collection):\n",
    "        self.db = db\n",
    "        self.col = col\n",
    "\n",
    "    @classmethod\n",
    "    def connect(cls, host: str, port=27017, db_name='wiki', col_name='articles'):\n",
    "        db = pymongo.MongoClient(host, port, unicode_decode_error_handler='ignore')[db_name]\n",
    "        return cls(\n",
    "            db=db,\n",
    "            col=db[col_name])\n",
    "\n",
    "    def get_articles(self, count=0) -> Generator:\n",
    "        return self.col.find({}).limit(count)\n",
    "\n",
    "    def get_article(self, title) -> dict:\n",
    "        doc = self.col.find_one({'title': title})\n",
    "        return doc if doc else {}\n",
    "\n",
    "    def get_articles_headings_texts(self, count=0) -> list:\n",
    "        for article in self.get_articles(count):\n",
    "            yield article['text']['Заголовок']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres storage - base class for all classes working with PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import psycopg2\n",
    "\n",
    "class PostgresStorage:\n",
    "    \n",
    "    conn: psycopg2.extensions.connection\n",
    "    \n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "\n",
    "    @classmethod\n",
    "    def connect(cls, \n",
    "                host: str, \n",
    "                port: int = 5432,\n",
    "                user: str = 'postgres',\n",
    "                password: str = 'password',\n",
    "                dbname: str = 'postgres'):\n",
    "        return cls(conn=psycopg2.connect(\n",
    "            host=host, port=port, user=user, password=password, dbname=dbname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habr storage - class for working posts from habr stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HabrStorage(PostgresStorage):\n",
    "\n",
    "    def get_posts(self, \n",
    "                  count: int = 0, \n",
    "                  habs_list: list = None, \n",
    "                  tags_list: list = None) -> Generator:\n",
    "        if not habs_list and not tags_list:\n",
    "            cursor = self.conn.cursor()\n",
    "            sql = 'SELECT * FROM posts'\n",
    "            if count:\n",
    "                sql += ' LIMIT %d' % count\n",
    "            cursor.execute(sql)\n",
    "            return (post for post in cursor.fetchall())\n",
    "        elif habs_list:\n",
    "            return self.__get_posts_by_habs(count, habs_list)\n",
    "        elif tags_list:\n",
    "            return self.__get_posts_by_tags(count, tags_list)\n",
    "\n",
    "    def get_posts_texts(self,\n",
    "                        count: int = 0,\n",
    "                        habs_list: list = None, \n",
    "                        tags_list: list = None) -> Generator:\n",
    "        posts_texts_gen = (post[2] for post in self.get_posts(count, habs_list, tags_list))\n",
    "        return posts_texts_gen\n",
    "\n",
    "    def __get_posts_by_habs(self, \n",
    "                            count: int,\n",
    "                            habs_list: list) -> Generator:\n",
    "        sql = '''SELECT P.* \n",
    "                   FROM posts P JOIN habs H ON P.post_id = H.post_id\n",
    "                  WHERE H.hab in (%s)''' % ''.join([\"'\" + str(hab) + \"', \" for hab in habs_list])[:-2]\n",
    "        sql = sql + \" LIMIT %d\" % count if count > 0 else sql\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        return (post for post in cursor.fetchall())\n",
    "\n",
    "    def __get_posts_by_tags(self, \n",
    "                            count: int,\n",
    "                            tags_list: list) -> Generator:\n",
    "        sql = '''SELECT P.* \n",
    "                   FROM posts P JOIN tags T ON P.post_id = T.post_id\n",
    "                  WHERE T.tag in (%s)''' % ''.join([\"'\" + str(tag) + \"', \" for tag in tags_list])[:-2]\n",
    "        sql = sql + \" LIMIT %d\" % count if count > 0 else sql\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        return (post for post in cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Iterable\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    to_sentences = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "    remove_brackets = re.compile(r' \\((.*?)\\)')\n",
    "    remove_punctuation = re.compile(r'[^a-zA-Zа-яА-Я ]')\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str, remove_punctuation=True, remove_brackets=True) -> Generator:\n",
    "        buf = text.split('\\n')\n",
    "        buf = (item for item in buf if item)\n",
    "        sentences = (sentence[:-1].lower().strip()\n",
    "                     for sentence in cls.to_sentences.split(' '.join(buf))\n",
    "                     if sentence[:-1])\n",
    "        if remove_brackets:\n",
    "            sentences = (cls.remove_brackets.sub('', sentence) for sentence in sentences)\n",
    "        if remove_punctuation:\n",
    "            return (cls.remove_punctuation.sub('', sentence) for sentence in sentences)\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    @classmethod\n",
    "    def get_sentences_gens(cls, texts: Iterable, remove_punctuation=True, remove_brackets=True) -> Generator:\n",
    "        for text in texts:\n",
    "            yield cls.tokenizer.tokenize(\n",
    "                text=text,\n",
    "                remove_punctuation=remove_punctuation,\n",
    "                remove_brackets=remove_brackets)\n",
    "\n",
    "    @classmethod\n",
    "    def get_text_gen(cls, text_gens_gen: Iterable) -> Generator:\n",
    "        for text_gen in text_gens_gen:\n",
    "            for sentences_gen in cls.get_sentences_gens(text_gen):\n",
    "                for sentence in sentences_gen:\n",
    "                    yield sentence.split()\n",
    "\n",
    "    @classmethod\n",
    "    def get_ngram_gen(cls, text_gens_gen: Iterable, ngram_size: int = 3) -> Generator:\n",
    "        for text_gen in text_gens_gen:\n",
    "            for sentences_gen in cls.get_sentences_gens(text_gen):\n",
    "                for sentence in sentences_gen:\n",
    "                    yield [''.join(item) for item in nltk.ngrams(sentence, ngram_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words encoder - class for encoding/decoding words stored as int nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Generator\n",
    "        \n",
    "class WordsEncoder:\n",
    "    \n",
    "    counter: int\n",
    "    word2int: dict\n",
    "    int2word: dict\n",
    "    begin_word: int = 0\n",
    "    end_word: int = -1\n",
    "        \n",
    "    def __init__(self, counter: int = None, word2int: dict = None, int2word: dict = None):\n",
    "        self.counter = counter\n",
    "        self.word2int = word2int\n",
    "        self.int2word = int2word\n",
    "\n",
    "    def fit(self, text_corpus):\n",
    "        self.counter = 0\n",
    "        self.word2int = {\n",
    "            self.begin_word: self.begin_word,\n",
    "            self.end_word: self.end_word\n",
    "        }\n",
    "        self.int2word = {\n",
    "            self.begin_word: self.begin_word,\n",
    "            self.end_word: self.end_word\n",
    "        }\n",
    "        for sentence in text_corpus:\n",
    "            for word in sentence:\n",
    "                if word not in self.word2int:\n",
    "                    self.counter += 1\n",
    "                    self.word2int[word] = self.counter\n",
    "                    self.int2word[self.counter] = word\n",
    "                    \n",
    "    def fit_encode(self, text_corpus) -> list:\n",
    "        corpus = list(text_corpus) if isinstance(text_corpus, Generator) else text_corpus\n",
    "        self.fit(corpus)\n",
    "        return self.encode_text_corpus_gen(corpus)\n",
    "\n",
    "    def encode_words_list(self, words_list: list) -> list:\n",
    "        return [self.word2int[word] for word in words_list]\n",
    "\n",
    "    def encode_text_corpus(self, text_corpus: list) -> list:\n",
    "        \"\"\"List of lists of words\"\"\"\n",
    "        return [self.encode_words_list(words_list) for words_list in text_corpus]\n",
    "\n",
    "    def encode_text_corpus_gen(self, text_corpus_gen: Generator) -> Generator:\n",
    "        \"\"\"List of lists of words\"\"\"\n",
    "        return (self.encode_words_list(sentence) for sentence in text_corpus_gen)\n",
    "\n",
    "    def decode_codes_list(self, codes_list: list) -> list:\n",
    "        return [self.int2word[code] for code in codes_list]\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Returns the underlying data as a Python dict.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"counter\": self.counter,\n",
    "            \"word2int\": self.word2int,\n",
    "            \"int2word\": self.int2word\n",
    "        }\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"\n",
    "        Returns the underlying data as a JSON string.\n",
    "        \"\"\"\n",
    "        return json.dumps(self.to_dict())\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, obj):\n",
    "\n",
    "        int2word = obj[\"int2word\"]\n",
    "        for key in int2word:\n",
    "            int2word[int(key)] = int2word.pop(key)\n",
    "\n",
    "        int2word[cls.end_word] = cls.end_word\n",
    "        int2word[cls.begin_word] = cls.begin_word\n",
    "\n",
    "        word2int = obj[\"word2int\"]\n",
    "        word2int[cls.end_word] = int(word2int.pop(str(cls.end_word)))\n",
    "        word2int[cls.begin_word] = int(word2int.pop(str(cls.begin_word)))\n",
    "\n",
    "        return cls(\n",
    "            counter=obj[\"counter\"],\n",
    "            word2int=word2int,\n",
    "            int2word=int2word\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_str):\n",
    "        return cls.from_dict(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder storage - class for working with words encoder stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStorage(PostgresStorage):\n",
    "    \n",
    "    model_name: str\n",
    "    begin_word: int = 0\n",
    "    end_word: int = -1\n",
    "        \n",
    "    def add_encoder(self, model_name: str, encoder: WordsEncoder):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL add_encoder(%s)', [model_name])\n",
    "        self.conn.commit()\n",
    "        \n",
    "        for code, word in encoder.int2word.items():\n",
    "            sql = f'''INSERT INTO {model_name}_encoder(code, word)\n",
    "                      VALUES (%s, %s)'''\n",
    "            cursor.execute(sql, [code, word])           \n",
    "        self.conn.commit()\n",
    "        self.__create_indexes(model_name)        \n",
    "            \n",
    "    def delete_encoder(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL delete_encoder(%s)', [model_name])\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def load_encoder(self, model_name: str) -> WordsEncoder:\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(f'SELECT code, word FROM {model_name}_encoder')\n",
    "        int2word = {}\n",
    "        word2int = {}\n",
    "        for row in cursor.fetchall():\n",
    "            code, word = row[0], row[1]\n",
    "            int2word[code] = word\n",
    "            word2int[word] = code\n",
    "        word2int[self.end_word] = int(word2int.pop(str(self.end_word)))\n",
    "        word2int[self.begin_word] = int(word2int.pop(str(self.begin_word)))\n",
    "        counter = len(int2word) - 2 # except begin and end words\n",
    "        return WordsEncoder(counter=counter,\n",
    "                            int2word=int2word,\n",
    "                            word2int=word2int)\n",
    "        \n",
    "    def __create_indexes(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL create_encoder_indexes(%s)', [model_name]);\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def __drop_indexes(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL drop_encoder_indexes(%s)', [model_name]);\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain storage - realization of markov chain stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "import bisect\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "def accumulate(iterable, func=operator.add):\n",
    "    it = iter(iterable)\n",
    "    total = next(it)\n",
    "    yield total\n",
    "    for element in it:\n",
    "        total = func(total, element)\n",
    "        yield total\n",
    "\n",
    "def compile_next(next_dict):\n",
    "    words = list(next_dict.keys())\n",
    "    cff = list(accumulate(next_dict.values()))\n",
    "    return [words, cff]\n",
    "\n",
    "\n",
    "class ChainStorage(PostgresStorage):\n",
    "    \n",
    "    begin_word: int = 0\n",
    "    end_word: int = -1\n",
    "        \n",
    "    def add_model(self, model_name: str, train_corpus: list, state_size: int):\n",
    "        model_dict = self.__build_model(train_corpus, state_size)\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL add_model(%s, %s)', [model_name, self.end_word])\n",
    "        self.conn.commit()\n",
    "        \n",
    "        for state_tuple in model_dict:\n",
    "            buf = model_dict[state_tuple]\n",
    "            choices_list, cumdist_list = buf[0], buf[1] \n",
    "            self.__add_state(cursor, model_name, state_tuple, choices_list, cumdist_list)            \n",
    "        self.conn.commit()\n",
    "        self.__create_index(model_name)\n",
    "        \n",
    "        del model_dict\n",
    "            \n",
    "    def delete_model(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL delete_model(%s)', [model_name])\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def walk(self, model_name: str, init_state: list, phrase_len: int = 10):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(f'SELECT chain_walk_{model_name}(%s, %s)', [init_state, phrase_len])\n",
    "        return cursor.fetchone()[0] or []\n",
    "    \n",
    "    def __build_model(self, train_corpus, state_size: int) -> dict:\n",
    "        model = {}\n",
    "\n",
    "        for run in train_corpus:\n",
    "            items = ([ self.begin_word ] * state_size) + run + [ self.end_word ]\n",
    "            for i in range(len(run) + 1):\n",
    "                state = tuple(items[i:i+state_size])\n",
    "                follow = items[i+state_size]\n",
    "                if state not in model:\n",
    "                    model[state] = {}\n",
    "\n",
    "                if follow not in model[state]:\n",
    "                    model[state][follow] = 0\n",
    "\n",
    "                model[state][follow] += 1\n",
    "                \n",
    "        model = { state: compile_next(next_dict) for (state, next_dict) in model.items() }\n",
    "        return model\n",
    "    \n",
    "    def __add_state(self,\n",
    "            cursor,\n",
    "            model_name: str,\n",
    "            state: tuple, \n",
    "            choices: list, \n",
    "            cumdist: list):\n",
    "        sql = f'''INSERT INTO {model_name}(state, choices, cumdist)\n",
    "                  VALUES (%s, %s, %s)\n",
    "                  ON CONFLICT DO NOTHING'''\n",
    "        cursor.execute(sql, [list(state), choices, cumdist])\n",
    "    \n",
    "    def __create_index(self, model_name: str, hash_index: bool = True):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL create_model_table_index(%s, %s)', [model_name, hash_index]);\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def __drop_index(self, model_name: str):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('CALL drop_model_table_index(%s)', [model_name]);\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_corpus = [\n",
    "    [1, 5, 6],\n",
    "    [65, 4, 1, 54],\n",
    "    [5, 65, 1, 324],\n",
    "    [3, 6, 54]\n",
    "]\n",
    "\n",
    "pg_model = ChainStorage.connect('172.17.0.2', dbname='markov')\n",
    "pg_model.add_model('test_sample', train_corpus=encoded_corpus, state_size=2)\n",
    "pg_model.walk('test_sample', [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_model.delete_model('test_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres chain usage with text processor & text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['барни', 'же', 'сидел', 'хмурый', 'и', 'о', 'чемто', 'думал', 'и', 'по']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_habs = HabrStorage.connect('172.17.0.3', dbname='habr')\n",
    "\n",
    "texts_list = list(pg_habs.get_posts_texts(10))\n",
    "train_corpus = list(TextProcessor.get_text_gen([texts_list,]))\n",
    "\n",
    "encoder = WordsEncoder()\n",
    "encoded_train_corpus = encoder.fit_encode(train_corpus)\n",
    "\n",
    "pg_model = ChainStorage.connect('172.17.0.2', dbname='markov')\n",
    "pg_model.add_model('another_test_sample', train_corpus=encoded_train_corpus, state_size=2)\n",
    "\n",
    "encoder.decode_codes_list(pg_model.walk('another_test_sample', [0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_model.delete_model('another_test_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generator model based on encoded markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterable\n",
    "\n",
    "class TextGenerator:\n",
    "    pg_chain: ChainStorage\n",
    "    pg_encoder: EncoderStorage\n",
    "    encoder: WordsEncoder\n",
    "    model_name: str\n",
    "    state_size: int\n",
    "    use_ngrams: bool\n",
    "    ngram_size: int\n",
    "    re_process: re.Pattern = re.compile(r'[^a-zA-Zа-яА-Я ]')\n",
    "\n",
    "    def __init__(self,\n",
    "                 pg_chain: ChainStorage,\n",
    "                 pg_encoder: EncoderStorage,\n",
    "                 model_name: str,\n",
    "                 state_size: int,\n",
    "                 input_text: Iterable = None,\n",
    "                 use_ngrams: bool = False,\n",
    "                 ngram_size: int = 3):\n",
    "        self.pg_chain = pg_chain\n",
    "        self.pg_encoder = pg_encoder\n",
    "        self.model_name = model_name\n",
    "        self.state_size = state_size\n",
    "        self.use_ngrams = use_ngrams\n",
    "        self.ngram_size = ngram_size\n",
    "\n",
    "        if input_text:\n",
    "            self.train_model(input_text)\n",
    "        else:\n",
    "            self.encoder = self.pg_encoder.load_encoder(model_name)\n",
    "            \n",
    "    def train_model(self, input_text: Iterable):\n",
    "        if self.use_ngrams:\n",
    "            train_corpus = list(TextProcessor.get_ngram_gen(input_text, self.ngram_size))\n",
    "        else:\n",
    "            train_corpus = list(TextProcessor.get_text_gen(input_text))\n",
    "\n",
    "        self.encoder = WordsEncoder()\n",
    "        encoded_train_corpus = self.encoder.fit_encode(train_corpus)\n",
    "\n",
    "        self.pg_encoder.add_encoder(self.model_name, self.encoder)\n",
    "        self.pg_chain.add_model(self.model_name, encoded_train_corpus, self.state_size)\n",
    "    \n",
    "    def delete_model(self):\n",
    "        self.pg_chain.delete_model(self.model_name)\n",
    "        self.pg_encoder.delete_encoder(self.model_name)\n",
    "        \n",
    "    def ngrams_split(self, sentence: str) -> list:\n",
    "        processed_sentence = self.re_process.sub('', sentence.lower())\n",
    "        ngrams_list = [''.join(item) for item in nltk.ngrams(processed_sentence, self.ngram_size)]\n",
    "        return ngrams_list\n",
    "\n",
    "    def words_split(self, sentence: str) -> list:\n",
    "        words_list = []\n",
    "        for word in sentence.split():\n",
    "            processed_word = self.re_process.sub('', word.lower())\n",
    "            if processed_word:\n",
    "                words_list.append(processed_word)\n",
    "        return words_list\n",
    "\n",
    "    def words_join(self, words_list: list) -> str:\n",
    "        return ' '.join(words_list)\n",
    "\n",
    "    def ngrams_join(self, ngrams_list: list) -> str:\n",
    "        return ngrams_list[0][:-1] + ''.join([ngram[-1] for ngram in ngrams_list])\n",
    "\n",
    "    def make_sentence(self, init_state: list, **kwargs):\n",
    "        tries = kwargs.get('tries', 10)\n",
    "        max_words = kwargs.get('max_words', None)\n",
    "        min_words = kwargs.get('min_words', None)\n",
    "        print(init_state)\n",
    "        \n",
    "        if init_state is not None:\n",
    "            init_state = self.encoder.encode_words_list(init_state)\n",
    "            prefix = init_state\n",
    "            for word in prefix:\n",
    "                if word == self.encoder.begin_word:\n",
    "                    prefix = prefix[1:]\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            prefix = []\n",
    "            \n",
    "        init_state = init_state[-self.state_size:]\n",
    "        print('prefix:', prefix)\n",
    "        print('init_state:', init_state)\n",
    "        for _ in range(tries):\n",
    "            codes_list = prefix + self.pg_chain.walk(self.model_name, init_state, 1000)\n",
    "            words_list = self.encoder.decode_codes_list(codes_list)\n",
    "            print(len(words_list))\n",
    "            if (max_words is not None and len(words_list) > max_words) or (\n",
    "                    min_words is not None and len(words_list) < min_words):\n",
    "                continue\n",
    "            if self.use_ngrams:\n",
    "                return self.ngrams_join(words_list)\n",
    "            return self.words_join(words_list)\n",
    "        return None\n",
    "\n",
    "    def make_sentence_with_start(self, input_phrase: str, **kwargs):\n",
    "        if self.use_ngrams:\n",
    "            items_list = self.ngrams_split(input_phrase)\n",
    "        else:\n",
    "            items_list = self.words_split(input_phrase)\n",
    "        items_count = len(items_list)\n",
    "\n",
    "        if items_count == self.state_size:\n",
    "            init_state = items_list\n",
    "\n",
    "        elif 0 < items_count:\n",
    "            init_state = [self.encoder.begin_word] * (self.state_size - items_count) + items_list\n",
    "        else:\n",
    "            init_state = [self.encoder.begin_word] * self.state_size\n",
    "\n",
    "        return self.make_sentence(init_state, **kwargs)\n",
    "\n",
    "    def make_sentences_for_t9(self, beginning: str, first_words_count=1, count=20) -> list:\n",
    "        phrases = set()\n",
    "        for i in range(count):\n",
    "            phrase = self.make_sentence_with_start(beginning)\n",
    "            if phrase:\n",
    "                words_list = phrase.split()\n",
    "                if len(words_list) > 1:\n",
    "                    phrases.add(\" \".join(words_list[first_words_count:]))\n",
    "        return list(phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_gen(\n",
    "        mongo_wiki: WikiStorage,\n",
    "        pg_habr: HabrStorage,\n",
    "        wiki_articles_count=1000,\n",
    "        habr_posts_count=1000,\n",
    "        **kwargs\n",
    "):\n",
    "    habr_posts_gen = pg_habr.get_posts_texts(\n",
    "        count=habr_posts_count, habs_list=kwargs.get('habs_list'), tags_list=kwargs.get('tags_list'))\n",
    "    wiki_articles_gen = mongo_wiki.get_articles_headings_texts(count=wiki_articles_count)\n",
    "    return (text_gen for text_gen in (habr_posts_gen, wiki_articles_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connections to dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_wiki = WikiStorage.connect(host='localhost')\n",
    "pg_habr = HabrStorage.connect(host='172.17.0.3', dbname='habr')\n",
    "pg_chain = ChainStorage.connect(host='172.17.0.2', dbname='markov')\n",
    "pg_encoder = EncoderStorage.connect(host='172.17.0.2', dbname='markov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.63 s, sys: 3.57 s, total: 9.2 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_corpus_gen = get_text_gen(mongo_wiki=mongo_wiki,\n",
    "                                pg_habr=pg_habr,\n",
    "                                wiki_articles_count=10,\n",
    "                                habr_posts_count=6000,\n",
    "                                habs_list=['Машинное обучение', 'Математика', 'Физика'])\n",
    "model = TextGenerator(pg_chain=pg_chain,\n",
    "                      pg_encoder=pg_encoder,\n",
    "                      model_name='habr',\n",
    "                      state_size=3,\n",
    "                      input_text=train_corpus_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(pg_chain=pg_chain,\n",
    "                      pg_encoder=pg_encoder,\n",
    "                      model_name='test_model',\n",
    "                      state_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'data engineering становится все более и более популярным keras за год догнал torch который разрабатывается уже лет судя по упоминаниям в научных статьях от до измерений причем выбор конкретного значения сводится к объему доступной памяти gpu',\n",
       " 'представляю вашему вниманию вторую часть статьи о поиске подозреваемых в мошениничестве на основе данных',\n",
       " 'представляю вашему вниманию перевод статьи everything you need to know about the android market how to get high rating on play store google play store eda',\n",
       " 'представляю вашему вниманию перевд статьи solving multiarmed bandits a comparison of epsilongreedy and thompson sampling',\n",
       " 'задача снижения размерности является одной из важнейших в анализе данных',\n",
       " 'сегодня мы продолжаем нашу классическую серию статей про то как автомобиль поедет в городских условиях']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.make_sentences_for_t9('привет хабр', first_words_count=2, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 16.5 s, total: 2min 27s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_corpus_gen = get_text_gen(mongo_wiki=mongo_wiki,\n",
    "                                pg_habr=pg_habr,\n",
    "                                wiki_articles_count=100,\n",
    "                                habr_posts_count=7000,\n",
    "                                habs_list=['Машинное обучение', 'Математика', 'Физика'])\n",
    "ngram_model = TextGenerator(pg_chain=pg_chain,\n",
    "                            pg_encoder=pg_encoder,\n",
    "                            model_name='ngram_size3_state3',\n",
    "                            state_size=3,\n",
    "                            input_text=train_corpus_gen,\n",
    "                            use_ngrams=True,\n",
    "                            ngram_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_model = TextGenerator(pg_chain=pg_chain,\n",
    "                            pg_encoder=pg_encoder,\n",
    "                            model_name='ngram_size3_state3',\n",
    "                            state_size=4,\n",
    "                            use_ngrams=True,\n",
    "                            ngram_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['хабр']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.make_sentences_for_t9('метод опорных векторов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['кто', 'тор', 'оро', 'ров']\n",
      "prefix: [1920, 358, 738, 320]\n",
      "init_state: [1920, 358, 738, 320]\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "init_state = list(ngram_model.ngrams_split('метод опорных векторов'))\n",
    "ngram_model.make_sentence(init_state[-4:], min_words=len(init_state) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model.pg_chain.walk(ngram_model.model_name, [1920, 358, 738, 320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
